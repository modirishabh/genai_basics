```markdown
# ЁЯЪА RAG (Retrieval-Augmented Generation) System

A Production-Ready RAG implementation using:

- **LangChain (JS)**
- **Google Gemini (Embeddings + LLM)**
- **Pinecone (Vector Database)**
- **Node.js**

---

# ЁЯУМ What is RAG?

RAG (Retrieval-Augmented Generation) is a technique where:

1. Relevant documents are retrieved from a vector database.
2. Retrieved content is added as context.
3. An LLM generates an accurate answer based only on that context.

This prevents hallucination and reduces token cost.

---

# ЁЯПЧ Architecture Overview

## Phase 1 тАУ Indexing (One-Time Setup)

```

PDF тЖТ Chunking тЖТ Embeddings тЖТ Vector DB

```

### Steps:
1. Load PDF
2. Split into chunks
3. Convert chunks into vectors
4. Store vectors in Pinecone

---

## Phase 2 тАУ Query (Runtime)

```

User Question
тЖУ
Query Embedding
тЖУ
Vector Search (Top K)
тЖУ
Create Context
тЖУ
LLM (Question + Context)
тЖУ
Final Answer

```

---

# ЁЯЫа Tech Stack

| Component | Tool |
|------------|------|
| LLM | Google Gemini |
| Embedding Model | text-embedding-004 |
| Vector Database | Pinecone |
| Framework | LangChain JS |
| Runtime | Node.js |

---

# ЁЯУВ Project Structure

```

project/
тФВ
тФЬтФАтФА index.js          # Indexing Phase
тФЬтФАтФА query.js          # Query Phase
тФЬтФАтФА DSA.pdf           # Sample Document
тФЬтФАтФА .env              # API Keys
тФЬтФАтФА package.json
тФФтФАтФА README.md

````

---

# тЪЩя╕П Setup Instructions

## 1я╕ПтГг Clone the Project

```bash
git clone <repo-url>
cd project
````

---

## 2я╕ПтГг Install Dependencies

```bash
npm install
```

Required packages:

```bash
npm install @langchain/community
npm install @langchain/textsplitters
npm install @langchain/google-genai
npm install @pinecone-database/pinecone
npm install dotenv
```

---

## 3я╕ПтГг Create `.env` File

```env
GOOGLE_API_KEY=your_google_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=your_index_name
```

---

# ЁЯЯв Phase 1: Indexing

Run once to create vector embeddings and store in Pinecone.

```bash
node index.js
```

### What Happens:

* Loads PDF
* Splits into 1000-character chunks
* Creates embeddings
* Stores vectors in Pinecone

---

# ЁЯФ╡ Phase 2: Query

Run interactive query system:

```bash
node query.js
```

Example:

```
What is Quick Sort?
Explain it in detail.
```

Supports follow-up questions using query rewriting.

---

# ЁЯза Production Enhancement: Query Rewriting

Follow-up questions like:

```
Explain it in detail
```

Are rewritten to:

```
Explain Quick Sort in detail
```

Using an additional LLM step before vector search.

This ensures correct retrieval context.

---

# ЁЯзй Important Concepts

## ЁЯФ╣ Chunking

* Chunk Size: 1000 characters
* Overlap: 200 characters
* Prevents context loss

---

## ЁЯФ╣ Embeddings

Text тЖТ Numerical Vector Representation

Used for semantic similarity search.

---

## ЁЯФ╣ Vector Search

Finds Top K semantically similar chunks.

---

## ЁЯФ╣ Hallucination Prevention

System Instruction:

* Only answer from retrieved context
* If not found тЖТ respond with:

  > "I could not find the answer in the provided document."

---

# ЁЯТ░ Cost Optimization

Instead of sending full document:

тЭМ Expensive
тЭМ Slow

RAG sends only relevant chunks:

тЬЕ Lower token usage
тЬЕ Faster response
тЬЕ Better accuracy

---

# ЁЯЪА Example Flow

### User asks:

```
What is AVL Tree?
```

System:

1. Converts query to embedding
2. Retrieves relevant chunks
3. Sends context + question to Gemini
4. Returns accurate answer

---

# ЁЯЫб Error Handling

If query not found in context:

```
Who is Elon Musk?
```

Response:

```
I could not find the answer in the provided document.
```

---

# ЁЯУК Why Not Fine-Tuning?

| Fine-Tuning      | RAG                  |
| ---------------- | -------------------- |
| Expensive        | Cost-efficient       |
| Requires GPU     | No retraining needed |
| Static knowledge | Dynamic knowledge    |
| Hard to update   | Easy to update       |

---

# ЁЯУИ Scaling

To scale this system:

* Increase Pinecone capacity
* Add streaming responses
* Add caching layer
* Add document metadata filtering
* Deploy on server (AWS / GCP / Vercel)

---

# ЁЯОп Use Cases

* Internal company chatbot
* HR policy assistant
* Legal document search
* Code documentation assistant
* Research paper Q&A

---

# ЁЯзк Future Improvements

* Add multi-document support
* Add hybrid search (keyword + semantic)
* Add evaluation metrics
* Add conversation memory persistence
* Add authentication layer

---

# ЁЯПБ Conclusion

This project demonstrates a **production-ready RAG pipeline** that:

* Prevents hallucination
* Supports follow-up questions
* Optimizes token usage
* Uses modern LLM infrastructure

---

# ЁЯУД License

MIT License

---

# ЁЯЩМ Credits

Built using:

* LangChain
* Pinecone
* Google Gemini
* Node.js

---

# тнР If this helped you

Give it a тнР on GitHub!

```
```
рдмрд╣реБрдд рдмрдврд╝рд┐рдпрд╛ ЁЯСМ рдЖрдкрдиреЗ рдкреВрд░рд╛ рдЯреНрд░рд╛рдВрд╕рдХреНрд░рд┐рдкреНрдЯ рд╢реЗрдпрд░ рдХрд┐рдпрд╛ рд╣реИред рдЕрдм рдореИрдВ рдЖрдкрдХреЛ **RAG (Retrieval-Augmented Generation)** рдХреЛ рдмрд┐рд▓реНрдХреБрд▓ рд╕рд╛рдл, structured рдФрд░ practical рддрд░реАрдХреЗ рд╕реЗ рд╕рдордЭрд╛рддрд╛ рд╣реВрдБ тАФ рддрд╛рдХрд┐ рдЖрдкрдХрд╛ concept crystal clear рд╣реЛ рдЬрд╛рдПред

---

# ЁЯФе 1. рд╕рдмрд╕реЗ рдкрд╣рд▓реЗ: RAG рдХреА рдЬрд╝рд░реВрд░рдд рд╣реА рдХреНрдпреЛрдВ рдкрдбрд╝реА?

### Problem: LLM тАЬрд╣реЗрд▓реБрд╕рд┐рдиреЗрдЯтАЭ рдХрд░рддрд╛ рд╣реИ

LLM (рдЬреИрд╕реЗ ChatGPT, Gemini) рдХрд╛ рдХрд╛рдо рд╣реИ рдЬрд╡рд╛рдм рджреЗрдирд╛ тАФ
рдЙрд╕реЗ тАЬрдирд╣реАрдВ рдкрддрд╛тАЭ рдмреЛрд▓рдирд╛ рд╕рд┐рдЦрд╛рдпрд╛ рд╣реА рдирд╣реАрдВ рдЧрдпрд╛ред

рдЙрджрд╛рд╣рд░рдг:

* рдЖрдкрдиреЗ рдкреВрдЫрд╛: тАЬрдореЗрд░реА рдЧрд░реНрд▓рдлреНрд░реЗрдВрдб рдХреЛ рдХреНрдпрд╛ рдкрд╕рдВрдж рд╣реИ?тАЭ
* рдЖрдкрдиреЗ рдЙрд╕реЗ рдХреЛрдИ рдбреЗрдЯрд╛ рдирд╣реАрдВ рджрд┐рдпрд╛
* рдлрд┐рд░ рднреА рд╡реЛ рдХреБрдЫ рднреА рдмрдирд╛ рджреЗрдЧрд╛

рдЗрд╕реЗ рдХрд╣рддреЗ рд╣реИрдВ: **Hallucination**

---

# ЁЯза 2. Solution рдХреНрдпрд╛ рд╣реЛ рд╕рдХрддрд╛ рд╣реИ?

рджреЛ рд░рд╛рд╕реНрддреЗ рд╣реИрдВ:

## тЭМ Option 1: Fine-Tuning

* рдЕрдкрдиреЗ рдбреЗрдЯрд╛ рдкрд░ рдкреВрд░рд╛ рдореЙрдбрд▓ рджреБрдмрд╛рд░рд╛ train рдХрд░реЛ
* рдорд╣рдВрдЧрд╛
* GPU рдЪрд╛рд╣рд┐рдП
* рд╕рдордп рд▓рдЧрддрд╛ рд╣реИ

## тЬЕ Option 2: RAG (Better Approach)

рд╣рд░ рд╕рд╡рд╛рд▓ рдкрд░:

1. Relevant data рдирд┐рдХрд╛рд▓реЛ
2. рдЙрд╕реЗ LLM рдХреЛ context рдХреЗ рд░реВрдк рдореЗрдВ рджреЛ
3. рдлрд┐рд░ рдЬрд╡рд╛рдм рдЬрдирд░реЗрдЯ рдХрд░рд╡рд╛рдУ

рдпрд╣реА рд╣реИ RAGред

---

# ЁЯУж 3. RAG рдХреНрдпрд╛ рд╣реИ?

RAG = Retrieval + Augmented + Generation

| Step         | рдХреНрдпрд╛ рд╣реЛрддрд╛ рд╣реИ           |
| ------------ | ---------------------- |
| Retrieval    | Relevant data рдЦреЛрдЬреЛ     |
| Augmentation | рдЙрд╕реЗ query рдХреЗ рд╕рд╛рде рдЬреЛрдбрд╝реЛ |
| Generation   | LLM рд╕реЗ answer рдмрдирд╡рд╛рдУ    |

---

# ЁЯПЧ 4. рдкреВрд░рд╛ рд╕рд┐рд╕реНрдЯрдо рджреЛ рдлреЗрдЬрд╝ рдореЗрдВ рдХрд╛рдо рдХрд░рддрд╛ рд╣реИ

---

## ЁЯЯв Phase 1: Indexing Phase (One-Time Setup)

рдпрд╣ рд╕рд┐рд░реНрдл рдПрдХ рдмрд╛рд░ рд╣реЛрддрд╛ рд╣реИред

### Step 1: Document рд▓реЛрдб рдХрд░реЛ

(PDF, DOC, Database, etc.)

### Step 2: Chunking рдХрд░реЛ

рдмрдбрд╝реЗ рдбреЙрдХреНрдпреВрдореЗрдВрдЯ рдХреЛ рдЫреЛрдЯреЗ рдЯреБрдХрдбрд╝реЛрдВ рдореЗрдВ рддреЛрдбрд╝реЛ

рдХреНрдпреЛрдВ?

* рдкреВрд░рд╛ рдбреЙрдХреНрдпреВрдореЗрдВрдЯ рднреЗрдЬреЛрдЧреЗ тЖТ рдЬреНрдпрд╛рджрд╛ tokens рдЦрд░реНрдЪ рд╣реЛрдВрдЧреЗ
* рд╕рд┐рд░реНрдл relevant рд╣рд┐рд╕реНрд╕рд╛ рдЪрд╛рд╣рд┐рдП

### Step 3: Embedding рдмрдирд╛рдУ

рд╣рд░ chunk рдХреЛ vector рдореЗрдВ рдмрджрд▓реЛ

Text тЖТ Numbers (vector)

Example:

```
"Quick sort is a sorting algorithm"
тЖТ [0.23, -0.91, 0.55, ...]
```

### Step 4: Vector Database рдореЗрдВ Store рдХрд░реЛ

(Pinecone, Milvus, Chroma)

рдЕрдм рдЖрдкрдХрд╛ data searchable рд╣реЛ рдЧрдпрд╛ semantic рддрд░реАрдХреЗ рд╕реЗред

---

## ЁЯФ╡ Phase 2: Query Phase (рд╣рд░ рдмрд╛рд░ рд╣реЛрддрд╛ рд╣реИ)

User рдкреВрдЫрддрд╛ рд╣реИ рд╕рд╡рд╛рд▓ред

### Step 1: Query рдХреЛ vector рдореЗрдВ рдмрджрд▓реЛ

### Step 2: Vector DB рдореЗрдВ search рдХрд░реЛ

Top 5 рдпрд╛ Top 10 similar chunks рдирд┐рдХрд╛рд▓реЛ

### Step 3: Context рдмрдирд╛рдУ

рдЙрди chunks рдХреЛ рдЬреЛрдбрд╝рдХрд░ context рдмрдирд╛рдУ

### Step 4: LLM рдХреЛ рджреЛ:

```
Question + Retrieved Context
```

### Step 5: LLM answer generate рдХрд░реЗрдЧрд╛

---

# ЁЯзи Important: Chunking рдореЗрдВ Overlap рдХреНрдпреЛрдВ?

рдЕрдЧрд░:

Chunk 1 тЖТ line 1тАУ1000
Chunk 2 тЖТ line 1001тАУ2000

рддреЛ рд╣реЛ рд╕рдХрддрд╛ рд╣реИ:

* Definition рдЖрдзреА рдкрд╣рд▓реЗ chunk рдореЗрдВ
* рдЖрдзреА рджреВрд╕рд░реЗ рдореЗрдВ

рдЗрд╕рд▓рд┐рдП overlap рдХрд░рддреЗ рд╣реИрдВ:

Chunk 1 тЖТ 1тАУ1000
Chunk 2 тЖТ 800тАУ1800

рддрд╛рдХрд┐ context рдЯреВрдЯреЗ рдирд╣реАрдВред

---

# ЁЯзй 5. Vector Database рдХреНрдпреЛрдВ рдЬрд░реВрд░реА?

рдЕрдЧрд░ рдЖрдкрдХреЗ рдкрд╛рд╕ 1000 pages рд╣реИрдВ, рддреЛ manually search рдХрд░рдирд╛ impossible рд╣реИред

Vector DB:

* Semantic search рдХрд░рддрд╛ рд╣реИ
* Similar meaning рдвреВрдВрдврддрд╛ рд╣реИ
* Exact keyword match рдирд╣реАрдВ

Example:
Query: "Explain quick sort deeply"

рдпрд╣ рд╕рдордЭ рдЬрд╛рдПрдЧрд╛:

* Quick sort
* Detailed explanation
* Sorting algorithm

---

# ЁЯЪи 6. рдмрдбрд╝рд╛ Production-Level Problem

Follow-up questions рдЯреВрдЯ рдЬрд╛рддреЗ рд╣реИрдВред

Example:

User:

> What is Quick Sort?

LLM:

> (Answer рджреЗрддрд╛ рд╣реИ)

User:

> Explain it in detail

рдЕрдм problem:
"Explain it" рдХрд╛ vector search рдХрд░рдиреЗ рд╕реЗ
Quick Sort рд╡рд╛рд▓рд╛ context рдирд╣реАрдВ рдорд┐рд▓реЗрдЧрд╛ред

---

# ЁЯза Advanced Solution (Production-Level RAG)

рдПрдХ extra LLM рд▓рдЧрд╛рддреЗ рд╣реИрдВ:

### Query Rewriter Model

рдпрд╣:

* Chat history рджреЗрдЦрддрд╛ рд╣реИ
* Follow-up question рдХреЛ standalone рдмрдирд╛рддрд╛ рд╣реИ

Example:

Input:

```
Explain it in detail
```

Rewrite:

```
Explain Quick Sort in detail
```

рдЕрдм vector search рд╕рд╣реА рдЪрд▓реЗрдЧрд╛ред

рдпрд╣ рд╣реИ production-ready RAGред

---

# ЁЯТ░ Token Cost Optimization

рдЕрдЧрд░ рдкреВрд░рд╛ document рд╣рд░ рдмрд╛рд░ рднреЗрдЬреЛрдЧреЗ:

тЭМ рдЬреНрдпрд╛рджрд╛ token рдЦрд░реНрдЪ
тЭМ slow response

RAG рдХреНрдпрд╛ рдХрд░рддрд╛ рд╣реИ?
тЬЕ рд╕рд┐рд░реНрдл relevant 5тАУ10 chunks рднреЗрдЬрддрд╛ рд╣реИ
тЬЕ рдХрдо token
тЬЕ рдХрдо cost
тЬЕ рдЬреНрдпрд╛рджрд╛ accuracy

---

# ЁЯЫа Tech Stack (рдЬреИрд╕рд╛ рд╡реАрдбрд┐рдпреЛ рдореЗрдВ рдерд╛)

* LangChain тЖТ Utility framework
* Google Embeddings тЖТ Text тЖТ Vector
* Pinecone тЖТ Vector DB
* Gemini тЖТ LLM

---

# ЁЯз╛ Final Flow Summary

## Phase 1 (One-Time Setup)

```
PDF тЖТ Chunk тЖТ Embedding тЖТ Vector DB
```

## Phase 2 (Every Query)

```
User Question
      тЖУ
Embedding
      тЖУ
Vector Search
      тЖУ
Top Chunks
      тЖУ
LLM (Question + Context)
      тЖУ
Answer
```

---

# ЁЯОп RAG рдХреЗ рдлрд╛рдпрджреЗ

тЬЕ Hallucination рдХрдо
тЬЕ Company data use рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ
тЬЕ Fine-tuning рдХреА рдЬрд░реВрд░рдд рдирд╣реАрдВ
тЬЕ Cost-efficient
тЬЕ Scalable

---

# ЁЯФе Simple Real-World Example

Imagine:

рдЖрдк HR рд╣реИрдВ
рдЖрдкрдХреЗ рдкрд╛рд╕ 500 policy documents рд╣реИрдВ

Employee рдкреВрдЫрддрд╛ рд╣реИ:

> What is maternity leave policy?

RAG рдХрд░реЗрдЧрд╛:

* 500 docs рдореЗрдВ рд╕реЗ relevant chunk рдирд┐рдХрд╛рд▓реЗрдЧрд╛
* LLM рдХреЛ рджреЗрдЧрд╛
* Proper answer рджреЗрдЧрд╛

LLM guess рдирд╣реАрдВ рдХрд░реЗрдЧрд╛ред

---

# ЁЯза One-Line Definition

> RAG is a method where we retrieve relevant documents from a vector database and provide them as context to an LLM to generate accurate answers.

---

рдЕрдЧрд░ рдЖрдк рдЪрд╛рд╣реЗрдВ рддреЛ рдореИрдВ:

* тЬФя╕П RAG vs Fine-Tuning comparison table рдмрдирд╛ рджреВрдБ
* тЬФя╕П Simple diagram рдмрдирд╛ рджреВрдБ
* тЬФя╕П Minimal RAG implementation pseudo-code рджреЗ рджреВрдБ
* тЬФя╕П Interview questions рдмрддрд╛ рджреВрдБ

рдмрддрд╛рдЗрдП рдЕрдм рдЖрдкрдХреЛ рдХрд┐рд╕ рд╣рд┐рд╕реНрд╕реЗ рдореЗрдВ рдФрд░ clarity рдЪрд╛рд╣рд┐рдП?
